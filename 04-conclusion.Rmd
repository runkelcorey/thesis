---
bibliography: bib/thesis.bib
csl: csl/chicago-note-bibliography.csl
output:
  pdf_document:
    keep_tex: true
  html_document: default
  word_document: default
---
# Effects of the Neighborhood Stabilization Program on Voting {#outcome}

With this model in hand, I turn now to its application.
My approach is simple but intensive: connect the votes for Republican House of Representatives candidates to foreclosure rates---taken with other variables as a rough proxy of home equity---at the Census tract level.
Logically, this approach nestles within that of Ansell 2014.
That paper connected home prices to local sentiments regarding government policies, and then looked at policies after those sentiment polls were conducted.
It imputed that, if there existed sentiments favoring less social insurance, followed by policies which enacted less social insurance, then those sentiments filtered through the voting and public opinion apparatus to produce those policies.
While Ansell's assumption that there were no significant confounding factors may have been correct, his methodology neglected the connective tissue between preferences and voting.
My approach searches for just that---votes associated with housing prices and/or foreclosure relief policies.

Preferences present in polling may be absent in voting for several reasons.
First, voters may not be faced with choices that reflect their preferences.
For instance, voters wishing to elect candidates focused on mortgage mortatoria may have had a suitable choice in 1930s Minnesota, but by 2010 that politics had all but disappeared.
Second, preferences may disappear under budget constraints.
Majoritarian voting asks the electorate to weigh preferences, as a single candidate in a two-party race is not likely to support every single preference that an individual would have.

Voter choice can thus be understood as a type of revealed preference: it obscures less-important preferences in favor of important distinctions.
Because of this fact, vote analysis is prone to false negatives---results that fail to detect preferences that actually exist.
Understanding these limitations will contextualize the conclusions I draw.
I cannot, for instance, reject the existence of preferences for more or less social insurance even among the sampled population; such preferences may be eclipsed by greater cultural or political factors.
I can, however, attempt to draw conclusions about the primacy of such preferences.
If there is robust evidence that housing prices affect voter behavior, then a conclusion may be that expected impacts to wealth play a *primary* role in deciding an individual's voting decisions, eclipsing other cultural or political factors.
Therefore, this argument cannot speak to Ansell's first step---that home prices affect social insurance preferences---but only to his second step---that such preferences affect policy outcomes---and only insofar as homeowners influence policy through voting (which is far from the only channel[@stern2011reassessing]).

```{r libs, include=FALSE}
library(tidyverse)
library(knitr)
library(stargazer)
library(kableExtra)
library(sf)
library(betareg)
options(tigris_class = "sf", scipen = 9999, digits = 2, knitr.graphics.auto_pdf = TRUE)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",")
})
stwide <- st_read("data/voting/display/wide.gpkg") %>%
  st_set_crs(4269)
```

Data complete with every variable in my model was available for more than half of the inhabited census tracts of 24 states, and greater than half of those census tracts where a self-described member of the Tea Party ran for election.
Most of the remaining census tracts belonged to states where it was not feasible to connect voting returns with census geography.
For instance, Michigan does not make clear the links between a voting tabulation district (VTD) as the Census Bureau defines it, and the county-precinct-ward system the state uses to publicize its results.
For 15 other states, including Illinois, elections are so devolved that a state-wide election aggregation would have taken days to collect.

Most of the data came by way of the US Census Bureau.
Since 1990, the Census Bureau has delineated census blocks covering the entire United States, standardizing units of analysis and making trivial the aggregation of data at various granularity.
However, some key variables in this analysis come from outside the Census Bureau, from state governments, other federal agencies, and independent researchers, each of which uses different geographical units.
To understand the construction of this novel dataset, it is important to first understand the relationships between different statistical geographies.

## Relationships between statistical geographies {#census-geography}
The basic Census geography runs as follows (see Figure \@ref(fig:geography) for the Census Bureau's graphical representation[@lemery2020united]).
The basic unit, blocks, were originally designed for Census takers to walk, and were constrained by physical boundaries difficult or illogical to cross.
However, since the Bureau undertook to cover the *entire* United States with census blocks, there are millions of census blocks with no inhabitants, covering lakes, parks, and barren landscapes; this fact will become important shortly.
Blocks are then aggregated into census tracts (my unit of analysis): tracts contain between 1,200 and 8,000 people, and are the smallest unit for which American Community Survey data---such as median household income and educational attainment---are published.
Census tracts sit inside counties and then inside states; unlike tracts and blocks, county boundaries are also legal jurisdictions and therefore rarely change.

```{r geography, out.width=c('50%','45%'), fig.align='center', fig.cap="Census geography relationships.", fig.subcap=c("To-scale breakdown of nested census geography, with street names.", "Organizational hierarchy of different geographies."), echo=FALSE, dev='pdf'}
include_graphics(c("figure/census_small_area_geography2.jpg", "figure/geo_hierarchy.png"))
```

While tracts and blocks were *designed* to be semi-permanent, and to act as the basis for other, non-Census areas, in practice their shapes shift with population changes.
These shifts mean that: not all 2000 tracts and 2010 tracts are exactly the same (though many are); and non-Census areas (such as state-designed VTDs) can cut census blocks.
Since these Census Bureau does not make public the responses of individual persons or homes until several decades after, placing each record in its appropriate area is impossible.
Rather, ecological methods approximate the distribution of persons, votes, or homes among geographies.
Ecological methods assume that persons, votes, or homes are equally distributed throughout an area: ecological methods would assume, for instance, that since six percent of Americans are millionaires, then six out of every hundred persons sampled---no matter their neighborhood---would be millionaires.
Strictly speaking, ecological methods are logical fallacies of de-composition, of taking the whole for the part.
However, guided by the right principles, ecological methods can be perfectly valid statistical tools.

The first principle is that smaller geographies are better.
Take for example tract 12099005947, a nearly-optimal tract of 4,406 people in Palm Beach County.
The owner occupation rate is 97%, and 88% of the residents are aged 65 or older.
However, Palm Beach County entire contains tracts with as low as 4% 65-and-up, and owner occupancy rates around 33%.
Taking the county for the tract would poorly represent the diversity of life within Palm Beach, smoothing out the nuances that differentiate one neighborhood's politics from the next.
Zooming in on census tracts improves the accuracy of ecological methods, allowing for a better picture

The second principle is that errors should be distributed evenly.
Errors here refers to the process by which some geographies must be decomposed and recomposed in order to translate non--2010 Census geography into 2010 Census geography.
This assumption may not always hold in my data: the history of American urban planning has shown that physical barriers---the same used to bound census blocks---often bind communities in more ways than one.
That being said, the supposed problem of partisan gerrymandering should be a moot point.
Gerrymandering operates on the state level, assigning particular voting precincts to particular Congressional districts.
Since my analysis deals with already-made and locally-created voting precincts, it skirts this particular problem.
I again rely on the granularity of this data for its validity, but without a secondary source of data (such as Zillow's or Redfin's proprietary sources), the accurate distribution of Neighborhood Stabilization Program data (which was enumerated in 2000 Census geography) will remain unknown.

The last principle is that data should be extensive.
This idea is native to all statistical inquiry, with the logic being that evenly-distributed errors will sort themselves out in large enough groups.
In this regard I have succeeded, with a 1/2 sample of the relevant elections totaling 27,618 census tracts.
To be fully open to criticism, however, I will describe the data aggregation and de/recomposition methods for each variable in my analysis.

## Data collection and aggregation
I built my data almost exclusively from official statistics and estimates.
Given that most of my sources would eventually lead back to the Census Bureau, which covers the entire United States, I knew that the factors limiting my coverage would lie outside the Census Bureau.
Collecting and translating precinct-level voting data to census tracts proved to be the most limited and time-consuming aspect of this process.
I began by downloading precinct-level voting returns from the Harvard Election Data Archive (HEDA),[@ansolabehere2014precinctlevel] started in 2008 by political science researchers.
This database compiled voting returns from most states and reported results for state and national races in a standardized format.

These figures were then wedded to files specifying the geographic boundaries of each voting precinct within a state.
At this stage saw the complete loss of several states' data as some systems used to identify precincts at the state-level were inconsistent with Census Bureau naming conventions.
For each state, I looked for an isomorphic mapping from state-level naming to the standardized naming of the Census Bureau or Harvard Election Data Archive, and only for links where the match was clear were the data used.
Case in point was Maryland: the voting returns data clearly contained every single specified tract in Maryland, but matching algorithms could only identify a few hundred of the several thousand voting districts.
Geographic files came primarily from the Census Bureau's TIGER/Line redistricting project, the official database for all Census geography,[@20122010] via the Election-Geodata mapping project, an open-source repository for redistricting data,[@kelso2020nvkelso] though, as alluded to above, some states' geographic files were also compiled by HEDA.

Translating the voting precincts to Census geography was a complicated process of decomposition and recomposition.
Since Census tracts often straddle two voting precincts, distributing votes to particular tracts relies on ecological methods, essentially peeling off the part of a precinct lying in one tract and stitching it together with parts of the other precincts that also coincide with a tract.
This process generates Frankenstein tracts, where 20% of one precinct's votes are summed with 100% of a second precinct's votes and 3% of a third precinct's votes, depending on how much of a precinct coincides with a tract.
Since the voting records of these tracts are purely statistical constructions, it is perfectly fine that one have fractions of votes.
Take again, for example, our tract 12099005947 in Palm Beach County Florida: it recorded 732.00 Republican votes out of 3114.00 total votes in the 2010 Congressional race.
But it's neighbor, tract 12099005949, recorded 461.85 Republican votes against 1414.25 total votes.
Figure \@ref(fig:palmbeach) shows how this geometry is possible.

```{r palmbeach, out.width='45%', fig.align='center', fig.cap="Voting precincts and census tracts in Palm Beach County, Florida.", fig.subcap=c("Without census tracts.", "With census tracts."), echo=FALSE, dev='pdf'}
include_graphics(c("figure/wotracts.png", "figure/withtracts.png"))
```


There are, however, more than one ways to cut a census tract.
The obvious way to slice census tracts is by geography: the spatial intersection forms the basis for dividing up votes.
The better way to slice census tracts is by voting-age population: the intersection of eligible people forms the basis for dividing up votes.
This method better leverages even more granular data that the Census Bureau publishes to partially overcome ecological approximations and account for differing population densities *within* tracts.
The way tracts in exurban and coastal areas are created necessitates this approach.
Ideal census tracts *are* fairly homogeneous in terms of density, as it is the goal of the Census Bureau to preserve neighborhoods and actual street blocks where possible.
In contrast, tracts containing blocks where population density begins to shift can be quite irregular.
Since tract size is constrained by population, tracts not only *can* subsume those zero-population blocks I mentioned earlier, but *must* subsume some uninhabited blocks.

Public Law 94-171 (PL 94-171) ensures that the Congressional districting mandate of equally-populated districts be met with small-area population counts.
In addition, requirements of the Voting Rights Act pressed states to consider the racial makeup of their Congressional districts.
Since these are governmental tabulations, the population counts were required to be public.
I drew from these data tables, comprising every block in the United States, in order to divide up census tracts.
The advantage from data granularity is large: half of the 11,166,336 census blocks in 2010 were smaller than a tenth of a square mile[@2015fcc, 1]---Charlottesville itself contains 803 census blocks, averaging 54.14 persons.[@20112010b]

Then, to distribute portions of voting precincts to tracts, I counted how many census blocks from each particular tract lay within each precinct.
For computational reasons, the relevant statistic here was the block's spatial center rather than its nuanced boundaries.
Since blocks are so small, and voting precincts are supposed to be fashioned from census blocks (known exceptions exist in a handful of states), using the entire boundaries of a block would likely have generated errors that had more to do with the resolution of downloadable mapping files than actual, in-practice boundaries.
Summing up the voting-age population (the census does not ask about citizenship status or felonious histories) of those blocks that lay within a particular precinct, and then comparing that figure to the total voting-age population of said precinct, allocates the percentage of votes from a precinct to a tract.
This method is represented formally in Equation \@ref(eq:vtd), where t represents a particular tract, p a precinct from the set P of precincts intersecting with that particular tract, and b a block from the set B of blocks within that particular precinct:

\begin{equation}
votes_{t} = \sum_{p=1}^{P \cap t} votes_p \cdot \frac{\sum_{b=1}^{B \cap t} VAP_b}{\sum_{b=1}^{B} VAP_b}
(\#eq:vtd)
\end{equation}

To recap, this process first decomposes precincts P into blocks B, and then recomposes precincts from tracts T each with particular allocations of votes according to their relative share of the total precinct population.

In this dataset, each tract's votes were calculated by this method, with the exception of Californian tracts.
The Redistricting Database for the State of California, hosted at the University of California Berkeley School of Law, had already performed a more precise version of this process using voting registration records.[@20112010]
Voter registration records allow researchers to locate each address---and thus each resident voter---precisely within tracts and precincts.
A more careful version of my analysis would implement such methods, as the time required to aggregate each state's registration records exceeded my timeframe.

A similar process of decomposition followed by recomposition was used for the Department of Housing and Urban Development's foreclosure and unemployment data,[@2008neighborhood; @cody2010neighborhood; @defilippo2009nsp2] which was tabulated at the 2000 tract- and block group--level.
The Census Bureau publishes relationship files that precisely place the percentage of housing units within each tract and block group in their 2010 equivalents.
I first decomposed 2000 tracts by the percentage of housing units intersecting with a particular 2010 tract, and then recomposed those 2010 tract figures by allocating foreclosure starts and total housing units according to the percentage each 2010 tract contained.

Lastly, data from the 2010 US Religion Census is collected only at the county-level.[@grammich2018religion]
Since tract geography follows county geography, I simply joined those county figures to each tract, so for each tract the county average is used.
All other data used were native to the 2010 tract level, without translation.
These data were primarily from the 2010 decennial census, with some additional information from the 2005-2009 American Community Survey.
American Community Survey data comes attached with margins of error, but, no other variables had these margins of error.
In the interest of time and observations, I took the Census Bureau at their word that errors were distributed evenly.

The only data natively at the 2010 tract level from outside the Census Bureau were house price indices calculated by the Federal Housing Finance Agency.
Those data were constructed from tens of millions of federally-financed home loans.
By virtue of that source, the indices are *only* composed of prime mortgages.
However, two factors work in my favor here.
First, significantly more prime mortgages than subprime mortgages defaulted during the foreclosure crisis.[@ferreira2015new]
Second, the FHFA's index only considers properties with at least 100 sales (and purchases).
This feature matters because housing prices are strongly spatial dependent: nearby homes with nonconforming mortgages will likely sell for similar amounts as homes with conforming mortgages.
Still, this limitation means that some tracts with high levels of nonconforming loans were ommitted.
There are many neighborhoods, for instance, with complete homogeneity of conforming or nonconforming loans, enclaves
often connected by mortgage brokers who handled large numbers of subprime loans or specific developments.[@dayenChainTitleHow2016]
The political behavior of these developments is interesting due to their uniformity, but beside the aims of this thesis.

## Data Analysis {#analysis}

My approach will be simple: I will model the full sample under the parameters specified in \@ref(model), and then compare it to models of Tea Party--candidate tracts, with the intent to whittle conclusions down to an appropriate level of generality.
In addition, I aim to persuade readers of the validity of these models, building on the case just made for why the underlying data should be accepted.

### Linear models

```{r fullsample, echo=FALSE, results='asis', eval=TRUE}
wide <- st_drop_geometry(stwide) #drop geometry for fast calculation
mod1 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W + STATE, wide)
mod2 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, wide) #run models
stargazer(mod1, mod2, type = "latex", title = "Full-sample linear models", omit = "^STATE", label = "tab:fullsample", font.size = "small", column.sep.width = "3pt", df = F, report = "vc*", header = F) #write table output
```

Table \@ref(tab:fullsample) lists the results of the full-sample linear models with and without state-level fixed effects.
Fixed effects are a sort of catch-all method to deal with unobserved characteristics particular to certain levels.
For instance, state laws governing taxation or the shape of voting precincts are not present in my dataset, so I employ state-level fixed effects to try and isolate these effects.
Mechanically, this method groups all observations belonging to each state as a single regressor.^[Due to the fact that 24 states are included in this analysis, I truncated the output in light of space. Almost all states had statistically-significant associated effects.]
While fixed-effects are standard fare in econometric analysis, I include both models because the assumptions of this fixed-effects model may not be met.
Namely, it presumes that all directly-relevant and observable predictors are present; as I will show later, these data may be too limited to fully explain the spatial correlation among the voting results.

The coefficients of most every control predictor are consistent with prior research, with two caveats.
Of particular importance, this model conforms with the literature connecting homeownership to conservative voting around the world, as the owner-occupied homeownership rate is positively associated with Republican vote share.
The 2010 unemployment rate and population density have the opposite signs that were expected: in my model, as the unemployment rate climbs, the Republican voting percentage falls; conversely, as the population density increases, so to does the vote share.
That being said, these are small effects; a (massive) 10-point rise in unemployment is associated here with just a 2.3-point rise in vote share.
For population density the impacts are even smaller.
The consistency of these impacts acts as a kind of eye test for the validity of this model, with the argument being that here are simply added predictors to an already-solid model.

Of interest to this thesis are the coefficients attached to the Neighborhood Stabilization Program, 2010 foreclosure rate, and change in home prices.
Against my hypothesis that foreclosure rates---a proxy for negative equity---would be negatively associated with Republican vote share, while home-price changes would be positively linked to Republican vote share, each regressor displays the *opposite* behavior.
Part of the price-change behavior seems to be absorbed by the state-level fixed effects, however.
The logic here is that there is correlation between Republican voting shares in nearby census tracts that covaries both with the particular state and with prices.
The nearness (or farness) of tracts is unobserved by this model, but may nonetheless contain important information linked to---but distinct from---price changes.
It is ambiguous what that information may be, however: it could align with financial data on equity or rates of seriously-delinquent mortgage payments, but alternatively, it could be socio-cultural, relating instead to irrational exuberance over price changes or perceptions of fortunate homeowners.

Separate from the effects of price changes and foreclosure rates, the Neighborhood Stabilization Program had a chronologically-dependent effect on Republican vote share.
Tracts targeted with the first round of grants (NSP1) were associated with a 1.75-point increase in Republican vote share, while the later rounds---NSP2 and NSP3---were associated with decreases in Republican vote shares after taking account of race, educational attainment, religious, and economic factors.
Since properties could not be purchased (let alone repaired or resold) before funds were granted, NSP3 (and to a lesser extent NSP2) tracts could not have seen financial benefit from the program.
I interpret this result as a strong evidence for the conclusion that the Neighborhood Stabilization Program, by supporting homeowner wealth, push voters rightward.
Why, though, would NSP2 and NSP3---without a robust advertising effort or partisan attachment---be associated with Democratic votes?
My theory is, again, unobserved variable bias.
NSP1 and NSP3 were targeted due to the risk of further foreclosures, associated with high-cost, high-leverage loans (most of which were subprime) and, therefore, mortgagees at the financial margins, who tend to vote more Democratic.

The effects of the Neighborhood Stabilization Program are sharpened by the presence of Tea Party politics.
Table \@ref(tab:justp) lists the results with and without state-level fixed effects for different subsets of the full sample, all of which saw races between self-avowed Tea Party candidates.
While the signs of coefficients attached to the NSP rounds remain the same, their significances vary.
This variance is likely due to sample size, as the number of tracts with winning candidates contained little more than a tenth of the total observations.
Throughout these models, most control variables remained entirely stable, with the unemployment rate and the proportion of residents 65 and older respectively losing and gaining statistical significance among models.

```{r justp, echo=FALSE, results='asis', eval=TRUE}
mod3 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W + STATE, filter(wide, RESULT != "EXTRA-TP"))
mod4 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, filter(wide, RESULT != "EXTRA-TP"))
mod5 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, filter(wide, RESULT == "LOSE"))
mod6 <- lm(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, filter(wide, RESULT == "WIN"))
stargazer(mod3, mod4, mod5, mod6, type = "latex", title = "Tea-Party linear models", omit = "^STATE", label = "tab:justp", font.size = "small", column.sep.width = "3pt", df = F, report = "vc*", header = F)
```

The variables of interest were a mixed bag.
NSP1 in each model showed strong statistical basis for rejecting the null hypothesis in favor of a positive effect on Republican voting, while the second and third rounds of the Neighborhood Stabilization Program consistently showed a negative association with Republican voting shares, though with inconsistent significance.
Due to the ever-shrinking size of the dataset, I lean towards chalking this result up to an issue of size.
The price change and foreclosure rates never delivered a statistically-significant pair consistent with expectations, with little evidence that either has a determinate effect on voting.

### Limitations and proposed refinements {#limitations}

As stated above, these models were limited by several factors.
The three areas I focus on as most important are data quality, model fitness, and spatial dependence.
My data quality concerns are ever-present in natural-experiment methodologies.
In short, individual voting characteristics, more granular price data, and a fuller spread of census tracts could have aided the conclusivity of this evidence.
Additionally, individual-level observations would have avoided the ecological assumptions this analysis rests on.

The second question, of model fitness, leaves for more ambiguous refinements.
In inferential statistics, there is less of a need for a model to fit the data comprehensively.
On that count, the attached .40-.62 R^{2} values are not concerning.
However, analysis of residuals leaves something to be desired.
Figure \@ref(fig:linearesid) shows the residuals against fitted values for the full-sample, fixed-effects model.
Ideally, residuals show no conclusive pattern, and are independent against the fitted values.
Here, there is a clear inverse relationship framed by the bounds of the Republican voting share at 0 and 100 percent.
Linear models are not built to handle bounded response variables, though I used one for the ease of computation and interpretation.

``` {r linear, echo=FALSE, out.width='45%', fig.align='center', fig.cap="Diagnostic plots of the linear models.", fig.subcap=c("Residuals of full linear model.\\label{fig:linearesid}","Acutal vs. fitted quantiles of full linear model.\\label{fig:linearquant}")}
include_graphics(c("figure/linearesid.png", "figure/linearquant.png"))
```

In order to validate the results of these models, I also modeled Republican vote share by different distributions.
I settled on reporting results for regressions on the Beta distribution because of its flexibility and ability to approximate the distribution of Republican voting share.
The Beta distribution is a basic (but complicated) distribution used for response variables bounded between 0 and 1.
It easily incorporates changing variance and does not assume linearity, which is helpful when considering social interactions generally, and electoral data specifically.
As Figure \@ref(fig:linearquant) shows, the variance of actual versus fitted values skyrockets at the upper end of my model.

``` {r betaresid, echo=FALSE, out.width='90%', fig.align='center', fig.cap="Residuals of beta regression."}
include_graphics("figure/betaresid.png")
```

In contrast, the behavior of Figure \@ref(fig:betaresid) conforms more closely to an ideal distribution, with less-pronounced upper and lower bounds.
Results of the beta regressions can be seen in Table \@ref(tab:beta).
Coefficients are altogether similar to what the linear models predict, making me more secure in my belief of their validity.

```{r beta, echo=FALSE, results='asis', eval=FALSE}
wide <- mutate(wide, RPCT = ifelse(RPCT < .05, RPCT + .05, ifelse(RPCT > .95, RPCT - .05, RPCT))/100)
mod7 <- betareg(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, wide)
mod8 <- betareg(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, dplyr::filter(wide, RESULT != "EXTRA-TP"))
mod9 <- betareg(RPCT ~ NSP + FORQ3*DENSITY + PRICECHG + VAP_B + EVANRATE + LDSRATE + VAP_H + VAP_H2 + MEDHHI + OWNEROCC + UNEM10 + OLD + WHITESOMECOL*VAP_W, filter(wide, RESULT == "LOSE"))
stargazer(mod7, mod8, mod9, type = "latex", title = "Beta regressions", omit = "^STATE", label = "tab:beta", font.size = "small", column.sep.width = "3pt", df = FALSE, report = "vc*", no.space = T, header = F)
```

Lastly, as mentioned, I believe these models were spatially-dependent, with biased the estimation of the home price effect on voting behavior.
I had originally planned to incorporate spatial autoregressive methods to correct for this spatial dependence, but limitations inherent to my data, as well as my own facility with such approaches, quashed that opportunity.
Nonetheless, I can virtually prove its existence in my models using standard tests of spatial correlation.

I first constructed a neighbor's matrix from geographic census tract files; this matrix tells **R** which tracts share an edge.
I then ran the residuals of each model (from a positivist interpretation, these are the unobserved variables) through Moran's I and Geary's C tests.
The similar tests look for spatial dependence respectively at the global and local levels.
Since this dataset is fractured, containing large discontiguities, Geary's C test will be the more appropriate measure; both are retained because the calculations are quick and similar.
Table \@ref(tab:spdep) displays the results of these tests, all of which reject the null hypothesis of no spatial dependence at the highest precision level **R** offers.

```{r spdep, echo=FALSE, message=FALSE}
spdep <- read_csv("data/voting/display/spdep.csv")
kable(spdep, "latex", booktabs = T, caption = "Results of various tests for spatial dependence. In all tests, the null hypothesis is spatially-independent.", align = 'c')
```

## Conclusion
In sum, there is strong evidence that recipients of the Neighborhood Stabilization Program's first round of grants voted more conservative in the 2010 elections, while subsequent rounds voted more liberal.
This fact can be explained by the timescale that the NSP operated on.
Other heretofore unexamined factors, such as home price level and foreclosure rate, show mixed results, though the strongest effects came as Tea Party politics stigmatized homeownership, foreclosure, and government relief programs.
