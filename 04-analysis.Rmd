# The Neighborhood Stabilization Program and Voting {#outcome}

\epigraph{No man who owns his own house and lot can be a Communist. He has too much to do.}{\textit{William Levitt}}

```{r libs, include=FALSE}
library(tidyverse)
library(knitr)
library(stargazer)
library(kableExtra)
library(sf)
library(betareg)
options(scipen = 9999, knitr.graphics.auto_pdf = TRUE)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",")
})
opts_chunk$set(cache = TRUE)
wide <- read_csv("wide.csv", col_types = cols(GEOID10 = col_character(), STATE = col_character()))
```

Complete data were available for 27,098 census tracts, covering more than half of the inhabited census tracts of 24 states, and more than half of the tracts where a self-described member of the Tea Party sought election.
Most of the census tracts missing belonged to states where the process of connecting voting returns with census geography was infeasible.
For instance, Michigan does not make clear the links between a voting tabulation district (VTD) as the Census Bureau defines it, and the county-precinct-ward system the state uses to publicize its results.
For 15 other states, including Illinois, elections are so devolved that a state-wide election aggregation would have taken days to collect.

While much of this analysis relies on data collected in 2010 census geographies, some key variables---including voting returns---were originally collected at a different geography.
Translating these data to the census-tract level was fundamental to my analysis, so it is important that I describe the relationships between statistical geographies as well as my process of translation.

## Relationships between statistical geographies {#census-geography}

Census geography runs as follows.
The basic unit, blocks, were originally designed just for Census takers to walk, and were constrained by physical boundaries difficult or illogical to cross.
However, since the Bureau undertook to cover the *entire* United States with census blocks, there are millions of census blocks with no inhabitants, covering lakes, parks, and barren landscapes; this fact will become important shortly.
Blocks are then aggregated into census tracts containing between 1,200 and 8,000 people.
Tracts are the smallest unit for which American Community Survey data---a continuously-run telephone survey that collects additional demographic data such as median household income and educational attainment---are published.
Census tracts sit inside counties and then inside states; unlike tracts and blocks, county boundaries are also legal jurisdictions and therefore rarely change.
See Figure \@ref(fig:geography) for the Census Bureau's graphical representation.[@lemery2020united]

```{r geography, out.width=c('50%','45%'), fig.align='center', fig.cap="Census geography relationships.", fig.subcap=c("To-scale breakdown of nested census geography, with street names.", "Organizational hierarchy of different geographies."), echo=FALSE, dev='pdf'}
include_graphics(c("figure/census_small_area_geography2.jpg", "figure/geo_hierarchy.png"))
```

While tracts and blocks were *designed* to be semi-permanent, acting as the basis for other, non-Census areas, in practice their shapes shift with population changes.
These shifts mean that: (a) not all 2000 tracts and 2010 tracts are congruent (though many are); and (b) non-Census areas (such as state-designed voting precincts) can cut census blocks.
Since these Census Bureau does not make public the responses of individual persons or homes until several decades after, placing each record in its appropriate area is impossible.
Rather, ecological methods approximate the distribution of persons, votes, or homes among geographies.
Ecological methods assume that persons, votes, or homes are equally distributed throughout an area: ecological methods would assume, for instance, that since six percent of Americans are millionaires, then six out of every hundred persons sampled---no matter their neighborhood---would also be millionaires.
Strictly speaking, ecological methods are logical fallacies of de-composition, of taking the whole for the part.
However, guided by the right principles, ecological methods can be perfectly valid statistical tools.

The first principle is that smaller geographies are better.
Take for example tract 12099005947, a nearly-optimal tract of 4,406 people in Palm Beach County.
The owner occupation rate is 97%, and 88% of the residents are aged 65 or older.
However, Palm Beach County entire contains tracts with as low as 4% 65-and-up, and owner occupancy rates around 33%.
Taking the county for the tract would poorly represent the diversity within Palm Beach, smoothing out the nuances that differentiate one neighborhood's politics from the next.
Zooming in on census tracts improves the accuracy of ecological methods, allowing for a fuller picture.

The second principle is that errors should be distributed evenly.
Errors here refer mainly to one party's votes that are misplaced among another party's voters, a result of the process by which voter tabulation districts must be decomposed and recomposed in order to translate non--2010 Census geography into 2010 Census geography.
This assumption may not always hold in my data: the history of American urban planning has shown that physical barriers---the same used to bound census blocks---often bind communities in more ways than one.
But since census tracts are also (if more weakly) constrained by physical boundaries, the aggregation of census blocks into tracts ought to be roughly consistent with the physical construction of communities by planners.
Additionally, most voting precincts do not cut tracts.
This means that the nightmare problem of gerrymandering deliberately undermining the assumption of evenly-distributed errors should be a moot point.
Gerrymandering divides *districts* into a biased collection of precincts, it does not redraw precinct lines.
Since my analysis deals with already-made and locally-created voting precincts, it skirts this particular problem potentially befalling analyses that begin at the county or Congressional-district level.

## Data collection and aggregation {#aggregation}

I built my data almost exclusively from official statistics and estimates.
Given that most of my sources would eventually lead back to the Census Bureau, which covers the entire United States, I knew that the factors limiting my coverage would lie outside the Census Bureau.
Collecting and translating precinct-level voting data to census tracts proved to be the most limited and time-consuming aspect of this process.
I began by downloading precinct-level voting returns from the Harvard Election Data Archive (HEDA).[@ansolabehere2014precinctlevel]
Started in 2008 by political scientists, this database compiled voting returns from most states and reported results for state and national races in a standardized format.

These figures were then wedded to files specifying the geographic boundaries of each voting precinct within a state.
At this stage saw the complete loss of several states' data as some systems used to identify precincts at the state-level were inconsistent with Census Bureau naming conventions.
For each state, I looked for an isomorphic mapping from state-level naming to the standardized naming of the Census Bureau or Harvard Election Data Archive, and only for links where the match was clear were the data used.
Case in point was Maryland: the voting returns data clearly contained every single specified tract in Maryland, but matching algorithms could only identify a few hundred of the several thousand voting districts.
Geographic files came primarily from the Census Bureau's TIGER/Line redistricting project, the official database for all Census geography,[@20122010] via the Election-Geodata mapping project, an open-source repository for redistricting data,[@kelso2020nvkelso] though, as alluded to above, some states' geographic files were also compiled by HEDA.

Translating the voting precincts to Census geography was a complicated process of decomposition and recomposition.
Since Census tracts often straddle two voting precincts, distributing votes to particular tracts relies on ecological methods, essentially peeling off the part of a precinct lying in one tract and stitching it together with parts of the other precincts that also coincide with a tract.
This process generates Frankenstein tracts, where 20% of one precinct's votes are summed with 100% of a second precinct's votes and 3% of a third precinct's votes, depending on how much of a precinct coincides with a tract.
Since the voting records of these tracts are purely statistical constructions, it is perfectly fine that one have fractions of votes.
Take again, for example, our tract 12099005947 in Palm Beach County Florida: it recorded 732.00 Republican votes out of 3114.00 total votes in the 2010 Congressional race.
But it's neighbor, tract 12099005949, recorded 461.85 Republican votes against 1414.25 total votes.
Figure \@ref(fig:palmbeach) shows how this geometry is possible.

```{r palmbeach, out.width='45%', fig.align='center', fig.cap="Voting precincts and census tracts in Palm Beach County, Florida.", fig.subcap=c("Without census tracts.", "With census tracts."), echo=FALSE, dev='pdf'}
include_graphics(c("figure/wotracts.png", "figure/withtracts.png"))
```

There is, however, more than one way to cut a census tract.
The obvious way to slice census tracts is by geography: the spatial intersection forms the basis for dividing up votes.
The better way to slice census tracts is by voting-age population: the intersection of eligible people forms the basis for dividing up votes.
This method better leverages even more granular data that the Census Bureau publishes to partially overcome ecological approximations and account for differing population densities *within* tracts.
The way tracts in exurban and coastal areas are created necessitates this approach.
Ideal census tracts *are* fairly homogeneous in terms of density, as it is the goal of the Census Bureau to preserve neighborhoods and actual street blocks where possible.
In contrast, tracts containing blocks where population density begins to shift can be quite irregular.
Since tract size is constrained by population, tracts not only *can* subsume those zero-population blocks I mentioned earlier, but *must* subsume some uninhabited blocks.

Public Law 94-171 (PL 94-171) ensures that the Congressional districting mandate of equally-populated districts be met with small-area population counts.
In addition, requirements of the Voting Rights Act pressed states to consider the racial makeup of their Congressional districts.
Since these are governmental tabulations, the population counts were required to be public.
I drew from these data tables, comprising every block in the United States, in order to divide up census tracts.
The advantage from data granularity is large: half of the 11,166,336 census blocks in 2010 were smaller than a tenth of a square mile[@2015fcc, 1]---Charlottesville itself contains 803 census blocks, averaging 54.14 persons.[@20112010b]

Then, to distribute portions of voting precincts to tracts, I counted how many census blocks from each particular tract lay within each precinct.
For computational reasons, the relevant statistic here was the block's spatial center rather than its nuanced boundaries.
Since blocks are so small, and voting precincts are supposed to be fashioned from census blocks (known exceptions exist in a handful of states), using the entire boundaries of a block would likely have generated errors that had more to do with the resolution of downloadable mapping files than actual, in-practice boundaries.
Summing up the voting-age population (the census does not ask about citizenship status or felonious histories) of those blocks that lay within a particular precinct, and then comparing that figure to the total voting-age population of said precinct, allocates the percentage of votes from a precinct to a tract.
This method is represented formally in Equation \@ref(eq:vtd), where t represents a particular tract, p a precinct from the set P of precincts intersecting with that particular tract, and b a block from the set B of blocks within that particular precinct:

\begin{equation}
votes_{t} = \sum_{p=1}^{P \cap t} votes_p \cdot \frac{\sum_{b=1}^{B \cap t} VAP_b}{\sum_{b=1}^{B} VAP_b}
(\#eq:vtd)
\end{equation}

To recap, this process first decomposes precincts into blocks, and then recomposes precincts from tracts (where the block-tract connection is trivial by construction).
Each tract is allocated votes according to its share of the total precinct population.

Each tract in this dataset was allocated votes by this method, with the exception of Californian tracts.
The Redistricting Database for the State of California, hosted at the University of California Berkeley School of Law, had already performed a more precise version of this process using voting registration records.[@20112010]
Voter registration records allow researchers to locate each address---and thus each resident voter---precisely within tracts and precincts.
A more careful version of my analysis would implement such methods, but the time required would have exceeded my timeframe.

A similar process of de/recomposition was used for the Department of Housing and Urban Development's foreclosure data,[@bucholtz2008neighborhood; @cody2010neighborhood; @richardson2009nsp2] which was tabulated at the 2000 tract- and block group--level.
While most of these tracts are the same, and almost all 2010 tracts only subdivide a single 2000 tract, updates to geographic surveying, changes in the landforms, and population fluctuations necessitate updated tracts.
The Census Bureau publishes relationship files that precisely place the percentage of housing units within each tract and block group in their 2010 equivalents.
Housing units were used here instead of persons since the relevant statistics concern properties, not people.
I first decomposed 2000 tracts by the percentage of housing units intersecting with a particular 2010 tract, and then recomposed those 2010 tract figures by allocating foreclosure starts and total housing units according to the percentage each 2010 tract contained.

Lastly, data from the 2010 US Religion Census[@grammich2018religion] and the Bureau of Labor Statistics[@2020local] were collected at the county-level.
Since tract geography follows county geography, I simply joined those county figures to each tract, so for each tract the county average was used.
All other data used were native to the 2010 tract level, without translation.
They came primarily from the 2010 decennial census,[@20112010b; @walker2020tigris] with some additional information from the 2005-2009 American Community Survey.[@201120062010; @walker2020tidycensus]
American Community Survey data comes attached with margins of error, but, as no other variables had these margins of error, they were entirely ignored so as not to complicate calculations.
In the interest of time and data *quantity*, I took the Census Bureau at their word that errors were distributed evenly.

Outside the Census Bureau, the Federal Housing Finance Agency located addresses within their 2010 tracts, requiring no further pre-processing.
Those data were constructed from tens of millions of federally-financed home loans.
By virtue of that source, the indices are *only* composed of prime mortgages.
However, two factors work in my favor here.
First, significantly more prime mortgages than subprime mortgages defaulted during the foreclosure crisis.[@ferreira2015new]
Second, the FHFA's index only considers properties with at least 100 sales (and purchases).[@bogin2019local]
This feature matters because housing prices are strongly dependent spatially: nearby homes with nonconforming mortgages will likely sell for similar amounts as homes with conforming mortgages.
Still, this limitation means that some tracts with high levels of nonconforming loans were omitted.
There are many neighborhoods, for instance, with complete homogeneity of conforming or nonconforming loans, enclaves connected by mortgage brokers who handled large numbers of subprime loans or specific developments.[@dayenChainTitleHow2016]
The political behavior of these developments is interesting due to their uniformity, but beside the aims of this thesis.

## Data Analysis {#analysis}

My approach is simple: I will test my assumption that the Neighborhood Stabilization Program increased housing prices; then, I will model the effect of the NSP on voting with and without home price controls.
If the NSP did raise home prices, then its presence ought to be associated with an increase in Republican voting over the model that controls for home prices, where no change in voting should be associated with the NSP.
Additionally, this section allows for the possibility that impacts on voting were sharpened where housing was politically stigmatized by Tea Party politics, as Chapter \@ref(motive-opportunity) suggested.
To evaluate those localized effects, trials are run that only include tracts in which a Tea Party candidate ran for office.

### Did NSP1 increase home prices?

Table \@ref(tab:prices) lists the effects of NSP1 on prices.
Additional models remove state-level fixed effects and variables that were included in the allocation formula for NSP1, and thus covary highly with which tracts actually received NSP1 funding.
To avoid price spillovers from interfering with measurement, I only included tracts which were at least 1 mile away from a tract receiving NSP1 funding.
The results are plain and robust to several possible covariates: tracts receiving NSP1 funding were associated with statistically significant home prices declines, estimated between one and two points lower, compared to tracts not receiving aid.

``` {r nsprice, echo=FALSE, results='asis', eval=TRUE}
mod1 <- lm(PRICECHG0710 ~ NSP1 + EMPLOYRISK + UNEM10 + MEDHHI + FORQ2*MORTOCC + FORQ3*MORTOCC + DENSITY + OLD + TENURE + TENURE2 + VAP_W + VAP_B + VAP_H + VAP_A + UNEMCHG + HICOST + SDQ + STATE, filter(wide, DISTNSP1 > 1, DISTNSP1 < 5))
mod2 <- lm(PRICECHG0710 ~ NSP1 + EMPLOYRISK + UNEM10 + MEDHHI + FORQ2*MORTOCC + FORQ3*MORTOCC + DENSITY + OLD + TENURE + TENURE2 + VAP_W + VAP_B + VAP_H + VAP_A + UNEMCHG, filter(wide, DISTNSP1 > 1, DISTNSP1 < 5))
mod3 <- lm(PRICECHG0710 ~ NSP1 + EMPLOYRISK + UNEM10 + MEDHHI + FORQ2*MORTOCC + FORQ3*MORTOCC + DENSITY + OLD + TENURE + TENURE2 + VAP_W + VAP_B + VAP_H + VAP_A + UNEMCHG + HICOST + SDQ + STATE, filter(wide, DISTNSP1 > 1, DISTNSP1 < 5, RESULT != "EXTRA-TP"))
stargazer(mod1, mod2, mod3, type = "latex", title = "Effect of NSP1 on Prices", omit = c("^STATE", "FORQ2:MORTOCC", "MORTOCC:FORQ3"), label = "tab:prices", df = F, align = T, report = "vc*", header = F, add.lines = list(c("State fixed Effects?", "Y", "N", "Y")), digits = 4, no.space = T, covariate.labels = c(NA, "Labor risk", "2010 unemployment rate", "Median household income", "2009 foreclosure rate", "Mortgaged occupancy rate", "2010 forelcosure rate", "Population density (per sq.mi.)", "65-and-up \\%", "Tenure in home", "Squared tenure", "White \\%", "Black \\%", "Hispanic \\%", "Asian \\%", "Unemployment change `05--`10", "High-cost mortgage rate", "Seriously delinquent rate"), dep.var.labels = "Home price index change, 2007-2010", column.sep.width = "2pt")
```

Results (not shown) of these models for NSP2 funding, allocated competitively within at-risk tracts, are similarly sobering.
The estimated negative impacts agree with results from a HUD evaluation of the Neighborhood Stabilization Program's second round of funding, which found insignificant-to-negative changes in home prices.[@spader2015evaluation]
Further, these models appear to be good predictors of home prices, accounting for 78% to 85% of total price variation among the 11,992 tracts included.
Diagnostic plots of the best model are available in Figures \@ref(fig:priceresid) and \@ref(fig:priceqq).

Differential effects of NSP1 in Tea Party tracts are explored in Model 3.
These results closely align with the full model augmented by state fixed effects and potential covariates, even retaining statistical significance despite only considering a fourth of the number of observations.
By any standard, the first round of NSP funding did not raise home prices.

While this evidence drives a nail into the coffin of foreclosure relief programs directly supporting anti-relief candidates, it still leaves open questions about the validity of the labor risk--equity model of voting explored in Chapters \@ref(motive-opportunity) and \@ref(methods).
Continuing the approach laid out above yields answers to two questions.
First, *could* relief programs directly aid the efforts of anti-relief candidates?
Second, in more general terms, are the effects of housing price changes magnified by low levels of home equity or high levels of labor risk?

### Voting Impacts of the NSP and Price Changes {#impacts}

Table \@ref(tab:fullsample) lists the results of the full-sample models with and without accounting for prices.
If the NSP's primary effect was lowering prices, then we should expect that tracts targeted by the first round of the Neighborhood Stabilization Program saw lower levels of Republican voting after taking into account other related factors except price changes.
Then, in the model *including* the price data, there should be no significant impact of NSP funding, as the only targets of the program were home prices.
For the labor risk--equity model, in all trials there should be positive correlations between price changes and Republican voting, with augmented values where price changes refract through economic anxiety, as measured by high foreclosure rates (i.e. low equity) or risky employment (a.k.a. high unemployment variance).

```{r fullsample, echo=FALSE, results='asis', eval=TRUE}
mod4 <- lm(RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_A + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, wide)
mod5 <- lm(RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK*ADJ_PRICE07 + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_A + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, wide)
stargazer(mod4, mod5, type = "latex", title = "Linear regression of NSP1 on Voting", label = "tab:fullsample", df = F, align = T, report = "vc*", header = F, digits = 4, no.space = T, covariate.labels = c(NA, "2010 forelcosure rate", "Labor risk", "Home price change `07--`10", "Unemployment change `05--`10", "Median household income (000s)", "Population density (per sq.mi.)", "Tenure in home", "Squared tenure", "65-and-up \\%", "Black \\%", "Hispanic \\%", "Hispanic \\% squared", "Asian \\%", "White \\%", "White: some college or less \\%", "Evangelical \\%", "L.D.S. \\%", "Cook PVI 2008", "Foreclosure rate $\\times$ labor risk", "Price change $\\times$ foreclosure rate", "Price change $\\times$ labor risk", "Foreclosure rate $\\times$ density", "White \\% $\\times$ some college or less", "Price change $\\times$ foreclosures $\\times$ risk"), dep.var.labels = "Republican vote percentage")
```

Consistent with expectations, the presence of the Neighborhood Stabilization Program is associated with a half point lower Republican voting in the model without prices and significant at the 5% level.
When taking into account prices, the presence of the Neighborhood Stabilization Program is not statistically significant, though still points negative.
While there are certainly reasons *not* to accept this model's validity, the consistency of the expectations with the model---and prior literature---is a good reason to believe in its capacity to describe impacts of economic and demographic variables on majoritarian voting behavior.
Down the line, from residential tenure through religious adherents, factors identified by prior research as impacting Republican turnout were similarly associated in my model.

Moreover, key features of the labor risk--equity model of political behavior found strong evidence in Model 5.
The interaction effects between price changes, foreclosure rates, and labor risk are of interest here (uninteracted forms are provided for completeness and their effects are not predicted by the labor risk--equity literature).
The magnifying effects of labor risk and foreclosure rate on price changes were directly proportional.
They were---in addition to most every other variable---statistically significant at the 1% level.
For instance, in our Palm Beach County tract 12099005947, my model estimated a <!--update-->5.7-point increase in Republican vote share due to the combination of home price decline, foreclosure rate, and variance of employment.
Both the triple interaction between prices, foreclosures, and labor risk, as well as the price change--foreclosure rate interaction hold positive coefficients, consistent with the model's prediction that low-equity and high-labor risk combine to decrease preferences for social insurance, tied in the United States to higher taxation.

Curiously, the pairwise interaction between price change and labor risk carries an inverse relationship with Republican voting.
In English, this result means that risky employment and increasing prices actually causes more to vote Democratic (or, technically, non-Republican) even as both labor risk and the triple interaction with prices and foreclosures are directly associated with Republican voting.
This result was robust to the addition and subtraction of several covariates and subsets of the observations (results not shown).
I account for this with post-crisis literature which argues that individuals with risky occupations face additional barriers to securing home equity lines of credit (HELOCs).[@mota2015spatial, Table 1]
Thus, rather than this negative association representing an inconsistency between predicted and actual effects of increased equity, it could be the case that incomplete credit markets prevented individuals with highly-risky employment from behaving as they would prefer.
In sum, the labor risk--equity model and prior literature on Republican voting behavior broadly agree with my regression results on the full sample of observations.

Because of these reasons, I urge acceptance of the model's capacity to understand the effects of certain economic variables on partisan voting behavior.
Here it is important to understand this specific sense of model validity.
The point here is not to predict vote outcomes, but to infer the effects of certain variables.
While prediction would be nice (and would be sufficient to validate a model's inferential power), all that need be true is that the unexplained variance be independent of the parameter(s) of interest.
For instance, there is evidence that subprime mortgage brokers lent to Black and Latin-American families at systematically higher rates than White families;[@dymski2013race; @faber2013racial] since race is also associated with Republican voting, I have included the relevant descriptive variables in my models.
Though I am worried about data quality with regards to ecological methods, the care taken to remove unobserved variable bias and the congruence of my results with expectations leads me to confidence in my *model*, and explains why I do not worry about fact that only half of the variation in Republican voting share is explained by my model.

How then did this model fare in Tea Party tracts?
In these next trials, more pronounced home price effects should be expected, though it may also be possible that "deadbeat" homeowners are further stigmatized by Tea Party candidates, leading to more conservatizing effects of increased foreclosure rates.
Table \@ref(tab:justp) shows the results of the same model above, but applied only to tracts in which a self-described member of the Tea Party ran for election.


```{r justp, echo=FALSE, results='asis', eval=TRUE}
mod6 <- lm(RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_A + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, filter(wide, RESULT != "EXTRA-TP"))
mod7 <- lm(RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK*ADJ_PRICE07 + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_A + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, filter(wide, RESULT != "EXTRA-TP"))
stargazer(mod6, mod7, type = "latex", title = "Linear regression of NSP1 on Voting", label = "tab:justp", df = F, align = T, report = "vc*", header = F, digits = 4, no.space = T, covariate.labels = c(NA, "2010 forelcosure rate", "Labor risk", "Home price change `07--`10", "Unemployment change `05--`10", "Median household income (000s)", "Population density (per sq.mi.)", "Tenure in home", "Squared tenure", "65-and-up \\%", "Black \\%", "Hispanic \\%", "Hispanic \\% squared", "Asian \\%", "White \\%", "White: some college or less \\%", "Evangelical \\%", "L.D.S. \\%", "Cook PVI 2008", "Foreclosure rate $\\times$ labor risk", "Price change $\\times$ foreclosure rate", "Price change $\\times$ labor risk", "Foreclosure rate $\\times$ density", "White \\% $\\times$ some college or less", "Price change $\\times$ foreclosures $\\times$ risk"), dep.var.labels = "Republican vote percentage")
```

These trials offer promising results but less statistical significance.^[The [unprinted] p-value attached to Model 6 is just above .10, nearly an academically-acceptable level of significance.]
First, the evidence that NSP1 had any effect on voting is far from conclusive, with statistically insignificant associations between round one funding and Republican vote shares.
Second, home price interaction terms are significant and carry the same signs as in trials run on the full slate of observations, but insignificant when uninteracted.
Third, the foreclosure rate switches signs when combined with the effects of home prices.
This switch means that *apart from their effects on prices* foreclosures are associated with increased Republican voting, but when those [negative] price effects are included, voters choose Democratic candidates more.
This finding agrees with the narrative that Tea Party candidates cast foreclosed homeowners as irresponsible and blame-worthy neighbors.

It seems that, since the NSP is associated with higher rates of Republican voting in the face of negative price effects, cultural (rather than economic) factors may have been at play.
One explanatory theory is the patronage model of voting, otherwise known as you scratch our back, and we'll scratch yours.
Table \@ref(tab:compnsp) compares how each of the NSP rounds fared with respect to Republican votes.
The first round saw statistically-insignificant effects on Republican voting, while rounds two and three saw decreases in Republican vote share of 3.2% and 6.0% respectively.
I excluded the second and third NSP rounds in my formal analysis because, as of July 2011, respectively 20.6% and 0.4% of funds had been expended from NSP2 and NSP3.[@185682011hud; @185682011huda]
Though NSP1 had expended 43.4% of funds and fully committed its allocations to grantees by September 2010,[@185682010huda] the analysis of housing prices above demonstrates that the program had not yet (if it had eventually) raised prices for recipients.
This fact should imply that even later rounds had less time to deploy price-stabilization measures.
Thus, any perceived effects could not been due to the price effects sought by the program, but rather by those factors which caused HUD to designate such tracts for funding, and I propose, by the designation itself.
By directing funding towards these neighborhoods, administrations *could* try to implement a licit form of vote buying.
That these effects would be so pronounced, and that residents would not also be motivated by an anti-welfare pride, is difficult to believe, however.

Explaining the cultural dynamics of this phenomenon (and teasing out its existence) opens opportunities for further research, but I present one theory now.
Eagle-eyed readers will have noted that, while the foreclosure rate and population density were included in Section \@ref(model), the interaction between them was left unexplained.
I have included this term to measure sociological effects of foreclosures, namely social contact theory, which posits that contact with an Other causes members of the in-group to sympathize with said Other.
In this case, that Other is foreclosed homeowners.
Where density is low, it may be possible to avoid seeing or knowing one of these "deadbeats", but as densities (and foreclosure rates) rise, it becomes less and less possible to assimilate ideas of irresponsibility and blame with people that one knows personally.
While the coefficient attached to the foreclosure rate--population density interaction term is small, it was statistically significant in every trial I ran.
Further, population density carries an enormous range of values; while most of these predictors are bounded between 0 and 100 percent, population density ranges from 0 to nearly 80,000.
As such, the interaction term ranges from 0 to `r range(wide$ADJ_FORQ*wide$DENSITY)[2]`, meaning that effects could be quite large at the upper end.


### Limitations and proposed refinements {#limitations}

However, even with all these statistically-significant terms, there are questions of model adequacy that I am poorly-equipped to address.
These models were limited by several factors, but the three areas I focus on as most important are data quality, model fitness, and spatial dependence.
My data quality concerns are ever-present in natural-experiment methodologies.
In short, individual voting characteristics, more granular price data, and a fuller spread of census tracts could have aided the conclusivity of this evidence.
Additionally, using individual-level demographics and vote results would have avoided the ecological assumptions this analysis rests on.

The second question, of model fitness, leaves for more ambiguous refinements.
In inferential statistics, there is less of a need for a model to fit the data comprehensively.
On that count, the attached .45-.59 $R^{2}$ values are not concerning.
However, further analysis of the residuals demonstrates weaknesses in model selection.
Figure \@ref(fig:linearesid) shows the residuals against fitted values for the full-sample, fixed-effects model.
Ideally, residuals show no conclusive pattern, and are independent against the fitted values.
Here, there is a clear inverse relationship framed by the bounds of the Republican voting share at 0 and 100 percent.
Linear models are not built to handle bounded response variables, though I used one for the ease of computation and interpretation.

``` {r linear, echo=FALSE, out.width='45%', fig.align='center', fig.cap="Diagnostic plots of the full linear model.", fig.subcap=c("Residuals on fitted values.\\label{fig:linearesid}","Acutal vs. fitted quantiles.\\label{fig:linearquant}")}
include_graphics(c("figure/linearesid.png", "figure/linearquant.png"))
```

In order to validate the results of these models, I also modeled Republican vote share by different distributions.
I settled on reporting results for regressions on the Beta distribution because of its flexibility and ability to approximate the distribution of Republican voting share.
The Beta distribution is a fundamental (but complicated) distribution used for response variables bounded between 0 and 1.
It easily incorporates changing variance and does not assume linearity, which is helpful when considering social interactions generally, and electoral data specifically.

``` {r betaresid, echo=FALSE, out.width='90%', fig.align='center', fig.cap="Standardized weighted residuals of full beta regression.", message=FALSE}
include_graphics("figure/betaresid.png")
```

Though there are still the artifacts of upper and lower bounds, the behavior of Figure \@ref(fig:betaresid) conforms more closely to an ideal distribution.
Results of the beta regressions can be seen in Table \@ref(tab:beta).
Coefficients are altogether similar^[For entirely computational reasons, the dependent variable, Republican voting percentage, had to be scaled to fit between 0 and 1. The predictors, therefore, were also reduced by a factor of 100 in order to remain comparable to the linear models. While this adjustment worked on all independent variables, the interaction terms---which had been products of numbers between 0 and 100---were then made to be products of numbers between 0 and 1. This translation is analogous to that from 10 to 0.1: $10 \times 10 = 100$, growing in magnitude, while $.1 \times .1 = .01$, shrinking by the same number of decimal places.] to what the linear models predict, making me more secure in my belief of their validity.

```{r beta, echo=FALSE, results='asis', eval=TRUE, warning=FALSE}
mod8 <- betareg(ADJ_RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_A + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, mutate_at(wide, vars(ADJ_FORQ, EMPLOYRISK, UNEMCHG, MEDHHI, DENSITY, TENURE, TENURE2, OLD, VAP_B, VAP_H, VAP_H2, VAP_W, VAP_A, WHITESOMECOL, EVANRATE, LDSRATE), funs(./100)))
mod9 <- betareg(ADJ_RPCT ~ NSP1 + ADJ_FORQ*EMPLOYRISK*ADJ_PRICE07 + UNEMCHG + MEDHHI + ADJ_FORQ*DENSITY + TENURE + TENURE2 + OLD + VAP_B + VAP_H + VAP_H2 + VAP_W*WHITESOMECOL + EVANRATE + LDSRATE + PVI08, mutate_at(wide, vars(ADJ_PRICE07, ADJ_FORQ, EMPLOYRISK, UNEMCHG, MEDHHI, DENSITY, TENURE, TENURE2, OLD, VAP_B, VAP_H, VAP_H2, VAP_A, VAP_W, WHITESOMECOL, EVANRATE, LDSRATE), funs(./100)))
stargazer(mod8, mod9, type = "latex", title = "Beta regression of NSP1 on Voting", label = "tab:beta", df = F, align = T, report = "vc*", header = F, digits = 4, no.space = T, covariate.labels = c(NA, "2010 forelcosure rate", "Labor risk", "Home price change `07--`10", "Unemployment change `05--`10", "Median household income (000s)", "Population density (per sq.mi.)", "Tenure in home", "Squared tenure", "65-and-up \\%", "Black \\%", "Hispanic \\%", "Hispanic \\% squared", "Asian \\%", "White \\%", "White: some college or less \\%", "Evangelical \\%", "L.D.S. \\%", "Cook PVI 2008", "Foreclosure rate $\\times$ labor risk", "Price change $\\times$ foreclosure rate", "Price change $\\times$ labor risk", "Foreclosure rate $\\times$ density", "White \\% $\\times$ some college or less", "Price change $\\times$ foreclosures $\\times$ risk"), dep.var.labels = "Republican vote percentage")
```

Lastly, as mentioned, these models were spatially-dependent, which biased the estimation of the home price effect on voting behavior.
Using state-level fixed effects and tract-level housing prices corrected some of this behavior, but the local nature of politics cannot be underestimated.
I had originally planned to incorporate spatial autoregressive methods to correct for this spatial dependence, but limitations inherent to my data, as well as a lack of remote access to better hardware, quashed that opportunity.^[This chapter was completed during 2020's novel coronavirus pandemic, which restricted access to University resources.]
Nonetheless, I can prove the presence of spatial dependence in my models to highlight future areas for exploration.

I first constructed a matrix of neighboring census tracts using geographic tract files; this matrix tells **R** which tracts are neighbors by triangulating their population-weighted geographic centers.
I then ran the residuals of each model (from a positivist interpretation, these are the unobserved variables) through Moran's I and Geary's C tests.
The similar tests look for spatial dependence respectively at the global and local levels.
Since this dataset is fractured, containing large discontiguities, Geary's C test will be the more appropriate measure; both are retained because the calculations are quick and similar.
Table \@ref(tab:spdep) displays the results of these tests, all of which reject the null hypothesis of no spatial dependence, affirming Tobler's first law of geography that "everything is related to everything else, but near things are more related than distant things."[@tobler1970computer, 236]

```{r spdep, echo=FALSE, message=FALSE}
spdep <- read_csv("data/voting/display/spdep.csv")
kable(spdep, "latex", booktabs = T, caption = "Results of various tests for spatial dependence. In all tests, the null hypothesis is spatial independence.", align = 'c', digits = 4)
```

## Conclusion

In sum, 2010 prices were negatively associated with the Neighborhood Stabilization Program's first round of funding.
Perhaps this result was due to unobserved variable bias, but, given the normalcy of regression diagnostics, and the results of later NSP rounds, it may instead result from the NSP's timeline.
Due to the legislative mandate that grantees purchase homes at a slight discount, there may not have been enough time to clear, repair, and re-appraise (or fully resell) properties by the 2010 midterms.
My results indicate that the NSP reduced prices between 1.3% and 2.0%, a small reduction keeping with size of the mandated discount.
Further, by NSP1's maturation in 2013, the amount of funds spent on acquisitions dropped, while new construction and rehabilitation increased slightly compared to its 2010 progress.[@185682010hud; @hud2013hud]

The labor risk--equity model was evaluated in spite of this fact, and evidence suggested the theorized effects were present in the 2010 midterms, though oddly not when only Tea Party tracts were considered.
Economic anxiety, composed of low equity and risky employment magnified the conservatizing effects of increased home prices.
These results were statistically significant and robust to several covariates.
Still to be explained are the mechanisms by which labor risk propagates through mortgage markets, HELOCs, and political behavior.
This variable proved "liberalizing", in tension with prior literature on the subject.

Limitations of the measurement and statistical methods were explored, with strong evidence that further work is needed to extricate the impacts studied from spatial dependence and bounded response assumptions.
Beta distribution regression was employed to correct for the latter issue, with slightly more ideal measures of model fitness and similarly strong evidence for the labor risk--equity model's capacity to explain voter behavior in the full sample as well as the Tea Party tracts.
These results build towards consensus with empirical literature, girding Ansell (2014) with voting results to provide a crucial middle step in his argument.
